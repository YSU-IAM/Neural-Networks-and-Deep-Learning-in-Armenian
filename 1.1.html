<!DOCTYPE html>
<html>
<head lang="en">
	<meta charset="UTF-8">
  <script type="text/x-mathjax-config;executed=true">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$']]},
      "HTML-CSS":
      {scale: 92},
      TeX: { equationNumbers: { autoNumber: "AMS" }}});
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link href="assets/style.css" rel="stylesheet">
  <link href="assets/pygments.css" rel="stylesheet">
  <link rel="stylesheet" href="http://code.jquery.com/ui/1.11.2/themes/smoothness/jquery-ui.css">
  <link rel="stylesheet" href="assets/s1.css">
	<title>Նեյրոնային ցանցեր և խորը ուսուցում: Մայքլ Նիլսենի գրքի թարգմանությունը հայերեն</title>
</head>
<body>
<h3>1.1. Պերսեպտրոններ</h3>

<p>Ի՞նչ է նեյրոնային ցանցը: Սկզբում ես կներկայացնեմ արհեստական նեյրոնի մի տարատեսակ, որ կոչվում է <em>պերսեպտրոն</em>: Պերսեպտրոնները <a href="http://books.google.ca/books/about/Principles_of_neurodynamics.html?id=7FhRAAAAMAAJ">ստեղծվել են</a> 1950-1960-ականներին <a href="http://en.wikipedia.org/wiki/Frank_Rosenblatt">Ֆրանկ Ռոզեսբլատի կողմից</a>, ոգեշնչված <a href="http://en.wikipedia.org/wiki/Warren_McCulloch">Ուորեն ՄակԿուլոքի</a> և <a href="http://en.wikipedia.org/wiki/Walter_Pitts">Վալտեր Փիթսի</a> ավելի վաղ կատարված <a href="http://scholar.google.ca/scholar?cluster=4035975255085082870">աշխատանքով</a>: Այսօր ավելի հաճախ օգտագործում են արհեստական նեյրոնների այլ մոդելներ․ այս գրքում և նեյրոնային ցանցերի վերաբերյալ ժամանակակից աշխատանքների մեծամասնության մեջ օգտագործվող նեյրոնների հիմնական մոդելը կոչվում է <em>սիգմոիդ նեյրոն</em>: Մենք շուտով կանդրադառնանք սիգմոիդ նեյրոններին: Բայց որպեսզի հասկանանք, թե ինչու են սիգմոիդ նեյրոնները սահմանվում այնպես, ինչպես սահմանվում են, արժե նախ ժամանակ ծախսել պերսեպտրոնները հասկանալու համար:</p>

<p>Ինչպե՞ս են աշխատում պերսեպտրոնները: Պերսեպտրոնը մուտքում ստանում է մի քանի երկուական արժեքներ,
	$x_1, x_2, \ldots$, և ելքում ստանում է մեկ երկուական արժեք․
<center>
	<img src="images/tikz0.png"/>
</center>
Այս օրինակում պերսեպտրոնը ունի երեք մուտքեր, $x_1, x_2, x_3$: Ընդհանուր դեպքում այն կարող է ունենալ ավելի շատ կամ ավելի քիչ մուտքեր: Ռոզենբլատը առաջարկել է ելքում ստացվող արժեքը հաշվարկելու պարզ կանոն: Նա ներմուծեց <em>կշիռներ</em>, $w_1,w_2,\ldots$, իրական թվեր, որոնք արտահայտում են համապատասխան մուտքերի կարևորությունը ելքի համար: Նեյրոնի ելքը, $0$ կամ $1$, որոշվում է կախված այն բանից, թե $\sum_j w_j x_j$ կշռված գումարը փոքր է, թե մեծ է որոշակի <em>շեմային արժեքից</em>: Շեմը, ինչպես կշիռները, իրական թիվ է, որը հանդիսանում է նեյրոնի պարամետր: Ավելի ճշգրիտ հանրահաշվական տերմիններով՝
<a class="displaced_anchor" name="eqtn1"></a>\begin{eqnarray}
\mbox{output} & = & \left\{ \begin{array}{ll}
0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}
\end{array} \right.
\tag{1}\end{eqnarray}

Այսքանն է պերսեպտրոնի աշխատանքի նկարագրությունը:</p>

<p>Սա պարզագույն մաթեմատիկական մոդելն է: Դուք կարող եք պերսեպտրոնը հասկանալ որպես մի սարք, որը փաստերը կշռելով կայացնում է որոշումներ: Քննարկենք մի օրինակ: Օրինակը այնքան էլ իրատեսական չէ, սակայն հեշտ է հասկանալը, և մենք շուտով կդիտարկենք ավելի իրատեսական օրինակներ: Ենթադրենք մոտենում են հանգստյան օրերը և դուք լսել եք, որ ձեր քաղաքում պանրի փառատոն է կայանալու: Դուք պանիր սիրում եք և ուզում եք որոշել արդյոք արժի գնալ փառատոնին: Դուք կայացնում եք որոշում հիմնվելով երեք գործոնների վրա․

<ol>
	<li> Եղանակը լա՞վն է,
	<li> Ձեր ընկերը կամ ընկերուհին ցանկություն ունե՞ն միանալ ձեզ,
	<li> Փառատոնին հնարավո՞ր է մոտենալ հասարակական տրանսպորտով (դուք չունեք ավտոմեքենա):
</ol>

Մենք կարող ենք այս երեք գործոնները ներկայացնել երկուական փոփոխականներով՝
$x_1, x_2$ և $x_3$: Օրինակ, եթե եղանակը լավն է, ապա ունենք $x_1 = 1$, իսկ եթե եղանակը բարենպաստ չէ, ապա $x_1 = 0$: Նմանապես, $x_2 = 1$ եթե ձեր ընկերը կամ ընկերուհին ցանկություն ունեն գնալու, և $x_2 = 0$ հակառակ դեպքում: Եվ նորից նման ձևով $x_3$-ը հասարակական տրանսպորտի հետ կապված:</p>

<p>Այժմ ենթադրենք որ դուք շատ եք սիրում պանիր, այնքան շատ, որ պարաստ եք գնալ փառատոնին նույնիսկ եթե ձեր ընկերը կամ ընկերուհին հետաքրքրված չեն և փառատոնին հասնելը դժվար է: Բայց գուցե դուք տանել չեք կարողանում վատ եղանակը և հաստատ չեք մասնակցի փառատոնին, եթե եղանակը անբարենպաստ եղավ: Այս բնույթի որոշման կայացումը մոդելավորելու համար կարող եք օգտագործել պերսեպտրոն: Օրինակ, կարելի է եղանակի համար կշիռը վերցնել $w_1 = 6$, իսկ մյուս պայմանների համար՝ $w_2 = 2$ և $w_3 = 2$: $w_1$-ի մեծ արժեքը ցույց է տալիս, որ եղանակը շատ կարևոր է ձեզ համար, շատ ավելի կարևոր է, քան արդյոք ձեր ընկերը կամ ընկերուհին կմիանան ձեզ, կամ հասարակական տրանսպորտի հարմարությունը: Վերջապես, ենթադրենք պերսեպտրոնի շեմը դուք ընտրում եք հավասար 5-ի: Այսպիսի ընտրության դեպքում պերսեպտրոնը իրականացնում է մեր ցանկացած որոշում կայացնող մոդելը, ելքում տալով 1, եթե եղանակը լավն է, և 0, եթե եղանակը բարենպաստ չէ: Ելքի վրա ընդհանրապես չեն ազդի ձեր ընկերոջ կամ ընկերուհու մասնակցելու ցանկությունը կամ հասարակական տրանսպորտի հարմարությունը:</p>


<p>Կշիռները և շեմը փոփոխելով մենք կստանանք որոշման կայացման տարբեր մոդելներ: Օրինակ, որպես շեմ ընտրենք $3$: Այդ դեպքում պերսեպտրոնը կորոշի որ դուք փառատոնին գնաք այն ժամանակ երբ եղանակը բարենպաստ է <em>կամ</em> երբ փառատոնը մոտ է հասարակական տրանսպորտին <em>և</em> ձեր ընկերը կամ ընկերուհին պատրաստ են միանալ ձեզ: Մի խոսքով դա կդառնա որոշում կայացնելու ուրիշ մոդել: Շեմն իջեցնելը նշանակում է որ դուք հակված եք փառատոնին մասնակցելուն:</p>

<p>Պարզ է որ պերսեպտրոնը մարդկային որոշում կայացնելու ամբողջական մոդել չէ: Սակայն օրինակը ցույց տվեց թե ինչպես այն կարող է համեմատել տարատեսակ գործոնները որոշում կայացնելու նպատակով: Ավելին, կարծես իրականալի է թվում այն որ պերսեպտրոնների բարդ կառուցվածքը կարող է անգամ իրականացնել ոչ պարզ որոշումներ:
<center>
	<img src="images/tikz1.png"/>
</center>
Հետևյալ ցանցում պերսեպտրոնների առաջին սյունակը, որին կանվանենք պերսեպտրոնների առաջին <em>շերտ</em>, իրականացնում է 3 պարզ որոշումներ` համեմատելով տրված գործոնները: Իսկ ի՞նչ կարելի է ասել 2-րդ շերտի պերսեպտրոնների մասին: Այդ պերսեպտրոններից յուդաքանչյուրը որոշում է կայացնում համեմատելով առաջին շերտի կայացրած որոշումների արդյունքները: Այդ կերպ երկրորդ շերտի պերսեպտրոնը կարող է կայացնել ավելի բարդ և աբստրակտ մակարդակի որոշում քան առաջին շերտի պերսեպտրոնները: Երրորդ շերտի պերսեպտրոնները կարող են կայացնել անգամ ավելի բարդ որոշումներ: Այս ձևով բազմաշերտ պերսեպտրոնների ցանցը կարող է կայացնել բավականին բարդ որոշումներ:</p>

<p>Incidentally, when I defined perceptrons I said that a perceptron has
	just a single output.  In the network above the perceptrons look like
	they have multiple outputs.  In fact, they're still single output.
	The multiple output arrows are merely a useful way of indicating that
	the output from a perceptron is being used as the input to several
	other perceptrons.  It's less unwieldy than drawing a single output
	line which then splits.</p><p>Let's simplify the way we describe perceptrons.  The condition $\sum_j
	w_j x_j > \mbox{threshold}$ is cumbersome, and we can make two
	notational changes to simplify it.
	The first change is to write
	$\sum_j w_j x_j$ as a dot product, $w \cdot x \equiv \sum_j w_j x_j$,
	where $w$ and $x$ are vectors whose components are the weights and
	inputs, respectively.  The second change is to move the threshold to
	the other side of the inequality, and to replace it by what's known as
	the perceptron's <em>bias</em>, $b \equiv
	-\mbox{threshold}$.  Using the bias instead of the threshold, the
	perceptron rule can be
	rewritten:
	<a class="displaced_anchor" name="eqtn2"></a>\begin{eqnarray}
	\mbox{output} = \left\{
	\begin{array}{ll}
	0 & \mbox{if } w\cdot x + b \leq 0 \\
	1 & \mbox{if } w\cdot x + b > 0
	\end{array}
	\right.
	\tag{2}\end{eqnarray}
	You can think of the bias as a measure of how easy it is to get the
	perceptron to output a $1$.  Or to put it in more biological terms,
	the bias is a measure of how easy it is to get the perceptron to
	<em>fire</em>.  For a perceptron with a really big bias, it's extremely
	easy for the perceptron to output a $1$.  But if the bias is very
	negative, then it's difficult for the perceptron to output a $1$.
	Obviously, introducing the bias is only a small change in how we
	describe perceptrons, but we'll see later that it leads to further
	notational simplifications.  Because of this, in the remainder of the
	book we won't use the threshold, we'll always use the bias.</p><p>I've described perceptrons as a method for weighing evidence to make
	decisions.  Another way perceptrons can be used is to compute the
	elementary logical functions we usually think of as underlying
	computation, functions such as <CODE>AND</CODE>, <CODE>OR</CODE>, and
	<CODE>NAND</CODE>.  For example, suppose we have a perceptron with two
	inputs, each with weight $-2$, and an overall bias of $3$.  Here's our
	perceptron:
<center>
	<img src="images/tikz2.png"/>
</center>
Then we see that input $00$ produces output $1$, since
$(-2)*0+(-2)*0+3 = 3$ is positive.  Here, I've introduced the $*$
symbol to make the multiplications explicit.  Similar calculations
show that the inputs $01$ and $10$ produce output $1$.  But the input
$11$ produces output $0$, since $(-2)*1+(-2)*1+3 = -1$ is negative.
And so our perceptron implements a <CODE>NAND</CODE>
gate!</p><p><a name="universality"></a></p><p>The <CODE>NAND</CODE> example shows that we can use perceptrons to compute
	simple logical functions.
	In fact, we can use
	networks of perceptrons to compute <em>any</em> logical function at all.
	The reason is that the <CODE>NAND</CODE> gate is universal for
	computation, that is, we can build any computation up out of
	<CODE>NAND</CODE> gates.  For example, we can use <CODE>NAND</CODE> gates to
	build a circuit which adds two bits, $x_1$ and $x_2$.  This requires
	computing the bitwise sum, $x_1 \oplus x_2$, as well as a carry bit
	which is set to $1$ when both $x_1$ and $x_2$ are $1$, i.e., the carry
	bit is just the bitwise product $x_1 x_2$:
<center>
	<img src="images/tikz3.png"/>
</center>
To get an equivalent network of perceptrons we replace all the
<CODE>NAND</CODE> gates by perceptrons with two inputs, each with weight
$-2$, and an overall bias of $3$.  Here's the resulting network.  Note
that I've moved the perceptron corresponding to the bottom right
<CODE>NAND</CODE> gate a little, just to make it easier to draw the arrows
on the diagram:
<center>
	<img src="images/tikz4.png"/>
</center>
One notable aspect of this network of perceptrons is that the output
from the leftmost perceptron is used twice as input to the bottommost
perceptron.  When I defined the perceptron model I didn't say whether
this kind of double-output-to-the-same-place was allowed.  Actually,
it doesn't much matter.  If we don't want to allow this kind of thing,
then it's possible to simply merge the two lines, into a single
connection with a weight of -4 instead of two connections with -2
weights.  (If you don't find this obvious, you should stop and prove
to yourself that this is equivalent.)  With that change, the network
looks as follows, with all unmarked weights equal to -2, all biases
equal to 3, and a single weight of -4, as marked:
<center>
	<img src="images/tikz5.png"/>
</center>
Up to now I've been drawing inputs like $x_1$ and $x_2$ as variables
floating to the left of the network of perceptrons.  In fact, it's
conventional to draw an extra layer of perceptrons - the <em>input
	layer</em> - to encode the inputs:
<center>
	<img src="images/tikz6.png"/>
</center>
This notation for input perceptrons, in which we have an output, but
no inputs,
<center>
	<img src="images/tikz7.png"/>
</center>
is a shorthand.  It doesn't actually mean a perceptron with no inputs.
To see this, suppose we did have a perceptron with no inputs.  Then
the weighted sum $\sum_j w_j x_j$ would always be zero, and so the
perceptron would output $1$ if $b > 0$, and $0$ if $b \leq 0$.  That
is, the perceptron would simply output a fixed value, not the desired
valued ($x_1$, in the example above). It's better to think of the
input perceptrons as not really being perceptrons at all, but rather
special units which are simply defined to output the desired values,
$x_1, x_2,\ldots$.</p><p>The adder example demonstrates how a network of perceptrons can be
	used to simulate a circuit containing many <CODE>NAND</CODE> gates.  And
	because <CODE>NAND</CODE> gates are universal for computation, it follows
	that perceptrons are also universal for computation.</p><p>The computational universality of perceptrons is simultaneously
	reassuring and disappointing.  It's reassuring because it tells us
	that networks of perceptrons can be as powerful as any other computing
	device.  But it's also disappointing, because it makes it seem as
	though perceptrons are merely a new type of <CODE>NAND</CODE> gate.
	That's hardly big news!</p><p>However, the situation is better than this view suggests.  It turns
	out that we can devise <em>learning
		algorithms</em> which can
	automatically tune the weights and biases of a network of artificial
	neurons.  This tuning happens in response to external stimuli, without
	direct intervention by a programmer.  These learning algorithms enable
	us to use artificial neurons in a way which is radically different to
	conventional logic gates.  Instead of explicitly laying out a circuit
	of <CODE>NAND</CODE> and other gates, our neural networks can simply learn
	to solve problems, sometimes problems where it would be extremely
	difficult to directly design a conventional circuit.</p>


</body>
</html>
