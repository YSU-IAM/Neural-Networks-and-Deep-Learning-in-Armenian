<!DOCTYPE html>
<html>
<head lang="en">
	<meta charset="UTF-8">
	<title>Նեյրոնային ցանցեր և խորը ուսուցում: Մայքլ Նիլսենի գրքի թարգմանությունը հայերեն</title>
</head>
<body>
<h1>Նեյրոնային ցանցեր և խորը ուսուցում</h1>

<p><i>«Նեյրոնային ցանցեր և խորը ուսուցում»</i>-ը ազատ առցանց գիրք է: Հեղինակն է <a href="http://neuralnetworksanddeeplearning.com/" target="_blank">Մայքլ Նիլսենը</a>:</p>

<ol>
	<li><a href="about.html">What this book is about</a></li>
	<li><a href="exercises.html">On the exercises and problems</a></li>
	<li>
		<a href="1.html">Using neural nets to recognize handwritten digits</a>
		<ol>
			<li><a href="1.1.html">Perceptrons</a></li>
			<li><a href="1.2.html">Sigmoid neurons</a></li>
			<li>The architecture of neural networks</li>
			<li>A simple network to classify handwritten digits</li>
			<li>Learning with gradient descent</li>
			<li>Implementing our network to classify digits</li>
			<li>Toward deep learning</li>
		</ol>
	</li>
	<li>
		<a href="2.html">How the backpropagation algorithm works</a>
		<ol>
			<li>Warm up: a fast matrix-based approach to computing the output from a neural network</li>
			<li>The two assumptions we need about the cost function</li>
			<li>The Hadamard product, s⊙t</li>
			<li>The four fundamental equations behind backpropagation</li>
			<li>Proof of the four fundamental equations (optional)</li>
			<li>The backpropagation algorithm</li>
			<li>The code for backpropagation</li>
			<li>In what sense is backpropagation a fast algorithm?</li>
			<li>Backpropagation: the big picture</li>
		</ol>
	</li>
	<li>Improving the way neural networks learn
		<ol>
			<li>The cross-entropy cost function</li>
			<li>Overfitting and regularization</li>
			<li>Weight initialization</li>
			<li>Handwriting recognition revisited: the code</li>
			<li>How to choose a neural network's hyper-parameters?</li>
			<li>Other techniques</li>
		</ol>
	</li>
	<li>
		A visual proof that neural nets can compute any function
		<ol>
			<li>Two caveats</li>
			<li>Universality with one input and one output</li>
			<li>Many input variables</li>
			<li>Extension beyond sigmoid neurons</li>
			<li>Fixing up the step functions</li>
			<li>Conclusion</li>
		</ol>
	</li>
	<li>
		Why are deep neural networks hard to train?
		<ol>
			<li>The vanishing gradient problem</li>
			<li>What's causing the vanishing gradient problem? Unstable gradients in deep neural nets</li>
			<li>Unstable gradients in more complex networks</li>
			<li>Other obstacles to deep learning</li>
		</ol>
	</li>
	<li>
		Deep learning
		<ol>
			<li>Convolutional neural networks</li>
			<li>Pretraining</li>
			<li>Recurrent neural networks, Boltzmann machines, and other models</li>
			<li>Is there a universal thinking algorithm?</li>
			<li>On the future of neural networks</li>
		</ol>
	</li>
	<li>Acknowledgements</li>
	<li>Frequently Asked Questions</li>

</body>
</html>
